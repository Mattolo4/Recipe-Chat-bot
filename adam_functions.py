import pandas as pd
import numpy as np
from collections import Counter
import re
import json
import random
import seaborn as sns
import matplotlib.pyplot as plt
import spacy
from langchain_community.llms import Ollama
from tqdm import tqdm
import ast

def load_data(path="RAW_recipes.csv", preprocess_steps=False):

    df_recipes = pd.read_csv(path)
    df_recipes['tags'] = df_recipes['tags'].apply(ast.literal_eval)
    df_recipes['ingredients'] = df_recipes['ingredients'].apply(ast.literal_eval)

    if preprocess_steps:
        df_recipes['steps'] = df_recipes['steps'].apply(ast.literal_eval)

    return df_recipes

def filter_tags(df, tags):
    df_filtered = df.copy()
    tags_set = set(tags)
    df_filtered = df_filtered[~df_filtered['tags'].apply(lambda x: bool(tags_set.intersection(set(x))))]
    
    return df_filtered

def filter_recipes_by_rating(recipes_df, interactions_df, min_num_ratings=3, min_mean_rating=2.5):
    # GENEROUSLY PROVDIDED BY CHATGPT

    # Load the data
    recipes_df = pd.read_csv('RAW_recipes.csv')
    interactions_df = pd.read_csv('RAW_interactions.csv')

    # Calculate the number of ratings and the average rating for each recipe
    ratings_summary = interactions_df.groupby('recipe_id')['rating'].agg(['count', 'mean']).reset_index()

    # Filter recipes with at least 3 ratings and an average rating greater than 2.5
    filtered_ratings = ratings_summary[(ratings_summary['count'] >= min_num_ratings) & (ratings_summary['mean'] > min_mean_rating)]

    # Merge the filtered ratings with the recipes DataFrame to get the final filtered recipes
    filtered_recipes = pd.merge(recipes_df, filtered_ratings, left_on='id', right_on='recipe_id')

    # Rename columns for clarity
    filtered_recipes.rename(columns={'count': 'num_ratings', 'mean': 'average_rating'}, inplace=True)

    # Save the filtered recipes to a new CSV file
    filtered_recipes.to_csv('filtered_recipes.csv', index=False)

    print("Filtered recipes with mean rating and number of ratings have been saved to 'filtered_recipes.csv'.")


def shuffle_and_split(data, train_ratio=0.87, val_ratio=0.03, test_ratio=0.1):
    # generated by ChatGPT!
    # Check if the ratios sum to 1
    assert train_ratio + val_ratio + test_ratio == 1.0, "The ratios must sum to 1"

    # Shuffle the data
    random.shuffle(data)

    # Calculate the split indices
    total_len = len(data)
    train_end = int(total_len * train_ratio)
    val_end = train_end + int(total_len * val_ratio)

    # Split the data
    train_data = data[:train_end]
    val_data = data[train_end:val_end]
    test_data = data[val_end:]

    return train_data, val_data, test_data

def list_to_string(lst):
    return ', '.join(lst)

def create_numbered_steps(lst):
    return '\n'.join([f"{i+1}. {step}" for i, step in enumerate(lst)])

def create_data_split_for_finetuning(df):
    json_list = []

    # Iterate through each row in the DataFrame
    for index, row in df.iterrows():
        ingredients_str = list_to_string(row['ingredients'])
        instructions_str = create_numbered_steps(row['steps'])
        
        json_object = {
            "instruction": "Give me a recipe I can make with the following ingredients.",
            "input": ingredients_str,
            "output": instructions_str
        }
        
        json_list.append(json_object)
    json_train, json_val, json_test = shuffle_and_split(json_list)

    # Save the list of JSON objects to a file
    with open('recipes_train.json', 'w') as json_file:
        json.dump(json_train, json_file, indent=4)
        
    with open('recipes_validation.json', 'w') as json_file:
        json.dump(json_val, json_file, indent=4)

    with open('recipes_test.json', 'w') as json_file:
        json.dump(json_test, json_file, indent=4)

    print("Generated JSON files 'recipes_train/validation/test.json' in the correct format for fine-tuning of Llama.")

def count_unique_characters(strings):
    char_count = {}
    for string in strings:
        for char in string:
            if char in char_count:
                char_count[char] += 1
            else:
                char_count[char] = 1
    return char_count

def preprocess_arr(arr_of_strings):
    chars_to_remove = ['!', '"', '%', '&', "'", '(', ')', '*', '-', '.', '/', '?', ',']

    for char in chars_to_remove:
        arr_of_strings = np.char.replace(arr_of_strings, char, '') # chatgpt helped

    pattern = r'\d' # chatgpt provided (I can't do even simple regex from head I am sorry)

    arr_of_strings = np.array([(re.sub(pattern, '', s)).lower() for s in arr_of_strings]) # get rid of numbers
    arr_of_strings = np.char.strip(arr_of_strings)  # get rid of leading and trailing white space
    return arr_of_strings

def get_word_sequences(text):
    return re.findall(r'\b[a-zA-Z]+\b', text)   # chatgpt

def white_list_user_input(white_list,user_input):

    white_list = np.array([' '+item+' ' for item in white_list])
    user_input = [' '+sent+' ' for sent in user_input]

    if isinstance(user_input, str):
        mask = np.array([user_input.find(s) != -1 for s in white_list]) #chatpgt

        result = white_list[mask]

        for j,ingredient1 in enumerate(result):
            for k,ingredient2 in enumerate(result):
                if j!=k and ingredient1 in ingredient2: 
                    if user_input[i].count(ingredient1) <= 1:
                        result = np.delete(result, j)

        result = np.char.strip(result)
        return result
    
    else:
        results = []
        for sentence in user_input:
            mask = np.array([sentence.find(s) != -1 for s in white_list]) #chatpgt
            results.append(np.char.strip(white_list[mask]))

        for i,result in enumerate(results):
            ingredients_to_delete = []
            for j,ingredient1 in enumerate(result):
                for k,ingredient2 in enumerate(result):
                    if j!=k and ingredient1 in ingredient2: 
                        if user_input[i].count(ingredient1) <= 1:
                            ingredients_to_delete.append(j)

            results[i] = np.delete(results[i], ingredients_to_delete)

        return results

def get_white_list(df, min_ingredient_appearances=10):
    ingredients = df["ingredients"]#.iloc[0:1000]
    all_ingredients = np.concatenate(ingredients)

    ingredients_white_list,counts = np.unique(all_ingredients,return_counts=True)
    ingredients_white_list = ingredients_white_list[counts > min_ingredient_appearances]
    #print(count_unique_characters(ingredients_white_list))
    ingredients_white_list = preprocess_arr(ingredients_white_list)
    #print(count_unique_characters(ingredients_white_list))

    return ingredients_white_list

def white_list(df, user_input):
    ingredients_white_list = get_white_list(df)
    user_input = preprocess_arr(user_input)
    return white_list_user_input(ingredients_white_list, user_input)

def ingredient_to_int(ingredients_dict, ingredients):
    result = [ingredients_dict.setdefault(i, -1) for i in ingredients]
    result.sort()
    return result

def encode_ingredients_df(df,ingredients_dict):
    df['ingredients_encoded'] = df['ingredients'].apply(lambda x: ingredient_to_int(ingredients_dict, x))
    return df

def encode_ingredients_arr(arr,ingredients_dict):
    arr_encoded = ingredient_to_int(ingredients_dict, arr)
    return arr_encoded

def levenstein_dist(arr1,arr2,score):

    if not len(arr1) and not len(arr2):
        # both are empty
        return score
    
    if not len(arr1):
        # one of them is empty
        return score+len(arr2)
    
    if not len(arr2):
        # one of them is empty
        return score+len(arr1)
    
    if arr1[0] == arr2[0]:
        return levenstein_dist(arr1[1:], arr2[1:], score)
    
    while True:
        if arr1[0] < arr2[0]:
            return levenstein_dist(arr1[1:], arr2, score+1)
        else:
            return levenstein_dist(arr1, arr2[1:], score+1)
    
def get_recipe_levenstein_dists(df,ingredients):
    df_vecs = df['ingredients_encoded']
    dists = df_vecs.apply(lambda x: levenstein_dist(x, ingredients, 0))
    return dists

def get_recipes_levenstein(df,user_ingredients,ingredients_dict,n):
    
    if not "ingredients_encoded" in df.columns:
        df = encode_ingredients_df(df,ingredients_dict)

    user_ingredients_encoded = encode_ingredients_arr(user_ingredients,ingredients_dict)

    dists = get_recipe_levenstein_dists(df,user_ingredients_encoded)

    top_recipes_ids = dists.sort_values()[:n].index
    #top_recipes_ids = df.iloc[top_recipes.index]["ingredients"].index
    return df.iloc[top_recipes_ids]

def get_ids_to_exclude(df, df_recipes_test):
    df_temp = df.copy()
    df_temp['steps_temp'] = df_temp['steps'].apply(create_numbered_steps)

    ids = []
    for i in range(len(df_recipes_test)):
        matches = df_temp[df_temp['steps_temp'] == df_recipes_test["output"][i]]
        id = list(matches["id"])
        ids += id
    
    return ids

def generate_violin_plot(data_dict):
    # CHATGPT GENERATED FUNCTION
    # Convert dictionary to a format suitable for seaborn
    data = []
    for method, measurements in data_dict.items():
        for measurement in measurements:
            data.append({'Method': method, 'Measurement': measurement})

    # Create a DataFrame from the data
    df = pd.DataFrame(data)

    # Set larger font sizes
    sns.set_context("notebook", font_scale=1.5)

    # Create the violin plot using seaborn
    plt.figure(figsize=(10, 6))
    sns.violinplot(x='Method', y='Measurement', data=df, inner='quartile', cut=0)
    plt.title('Ingredients extraction method comparison')
    plt.xlabel('Method')
    plt.ylabel('Levenstein distance')
    plt.xticks(fontsize=14)
    plt.yticks(fontsize=14)
    plt.show()

def extract_ingredients_NER(user_input, model_path="ner_model"):
    nlp = spacy.load(model_path)
    results = []
    for sentence in user_input:
        doc = nlp(sentence)
        result = list(doc.ents)
        results.append([str(s).lower() for s in result])
    return results

def extractIngredients(sentences):
    llm = Ollama(model="llama3")
    all_ingredients = []

    for sentence in tqdm(sentences):
        output = ''
        prompt = f"Output the ingredients from the following sentence without any comments, separated only by commas: {sentence}"
        for chunks in llm.stream(prompt):
            output += chunks

        #pattern = r'\b[a-zA-Z]+\b'  # Simple pattern to match words
        #ingredients = re.findall(pattern, output.lower())
        all_ingredients.append(output)
    
    return all_ingredients

def delete_before_last_newline(s):
    # CHATGPT
    # Find the position of the last newline character
    last_newline_pos = s.rfind('\n')
    
    # If a newline is found, return the substring after it
    if last_newline_pos != -1:
        return s[last_newline_pos + 1:]
    else:
        # If no newline is found, return the original string
        return s
    
def llm_results_postprocess(llm_results):
    llm_results_split = [list(map(lambda x: x.lower().strip(), llm_results[i].split(','))) for i in range(len(llm_results))]
    llm_results_clean = []
    for s in llm_results_split:
        s_clean = [delete_before_last_newline(ss) for ss in s]
        llm_results_clean.append(s_clean)
        return llm_results_clean